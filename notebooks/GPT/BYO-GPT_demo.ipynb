{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c094c805-db9c-4aa9-a412-d6f8a5d4ca6f",
   "metadata": {},
   "source": [
    "# Build your own GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e14a7-e7da-470f-80f4-312eae6c7491",
   "metadata": {},
   "source": [
    "### 1. Choose Training and Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed022d5-aab3-4e1e-ab0e-b2b4d5afbc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117a80410>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Training hyperparameters\n",
    "# Number of independent sequences will we process in parallel\n",
    "batch_size = 16 \n",
    "# Maximum context length for predictions\n",
    "block_size = 32\n",
    "# Number of training iterations\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "#To use a GPU or not!\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "\n",
    "\n",
    "#Model hyperparameters\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0\n",
    "# ------------\n",
    "\n",
    "# Setting a random seed for reproducibility\n",
    "torch.manual_seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f3e27d-d618-46d2-ae82-290666a1ad60",
   "metadata": {},
   "source": [
    "### 2. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37165f66-aca3-4228-9d6f-43e0ac07fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/2600/pg2600.txt\"\n",
    "\n",
    "response = request.urlopen(url)\n",
    "raw = response.read()\n",
    "text = raw.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b0ae2-3db8-4520-baec-f289eb040f80",
   "metadata": {},
   "source": [
    "### 3. Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c546e22-d20b-4c30-ae8d-189908b67080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our Vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# Encode the data\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Create train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # 9:1 train test split\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Batching!\n",
    "def get_batch(split):\n",
    "    # create a batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e3088-4489-4642-ab18-c7aa0b9607c3",
   "metadata": {},
   "source": [
    "### 4. Transformer Ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb61c76f-94cd-47db-a4d8-aba31ebcb102",
   "metadata": {},
   "source": [
    "1. Setting up the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94ab85f7-a288-4648-a3c6-d855f1382322",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633aa78a-3dbe-4eab-b53d-15a96ef0c7a6",
   "metadata": {},
   "source": [
    "2. Setting up the single headed attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "409b5f12-89e9-47f3-838c-465830dc275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d33cb-6c5e-4557-a450-8d65a8c5f873",
   "metadata": {},
   "source": [
    "3. Setting up a class for multiple attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a89bf84-50b8-414a-8765-1ca1b8f48820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6439c47-8edf-4c5e-ba67-1b850cf4a0a0",
   "metadata": {},
   "source": [
    "4. Setting up the feedforward layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a51a0e7a-ce03-45c0-96c5-d3e8dd6accca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139bd293-b48f-4199-b6b4-aa9d719e9292",
   "metadata": {},
   "source": [
    "### 5. Assembling the Generative Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3827369-0b56-4982-965d-109c03787d90",
   "metadata": {},
   "source": [
    "1. Setting up the transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be2b1408-bdfd-4791-a347-4f9ceed6c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53b24dc-52fe-47cd-a026-f6ea95afde1b",
   "metadata": {},
   "source": [
    "2. Create a simple bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be7764c0-e25c-49d6-a2ef-725a9b92d0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70ce52-a914-4beb-95ed-ba38a4055eda",
   "metadata": {},
   "source": [
    "### 6. Train the Transformer on Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293905a-2b0b-4495-bf87-2b3e9fc2c423",
   "metadata": {},
   "source": [
    "Calculate the total number of parameters in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4893329c-317f-40f9-9893-c58ed33b6d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.215921 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155eab44-64d4-435a-aeda-1dcc3ba0f1b7",
   "metadata": {},
   "source": [
    "Setup the PyTorch optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59521410-f569-4ce0-9a6e-ce58cfd69c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nityamandyam/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4a3f10-8060-4c7e-8ac0-9a31ff531b56",
   "metadata": {},
   "source": [
    "It's finally time to train the transformer on our dataset! We're going to use Python's `timeit` library to evaluate the time it takes to pretrain our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93466b34-9556-4d2a-8d97-d4e5756fbe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting time: 13.435623041\n",
      "step 0: train loss 4.9321, val loss 4.9300\n",
      "step 100: train loss 2.6375, val loss 2.6250\n",
      "step 200: train loss 2.4884, val loss 2.4790\n",
      "step 300: train loss 2.4422, val loss 2.4434\n",
      "step 400: train loss 2.3417, val loss 2.3535\n",
      "step 500: train loss 2.2662, val loss 2.2832\n",
      "step 600: train loss 2.2147, val loss 2.2301\n",
      "step 700: train loss 2.1631, val loss 2.1869\n",
      "step 800: train loss 2.1232, val loss 2.1409\n",
      "step 900: train loss 2.0765, val loss 2.0916\n",
      "step 1000: train loss 2.0199, val loss 2.0589\n",
      "step 1100: train loss 2.0109, val loss 2.0402\n",
      "step 1200: train loss 1.9746, val loss 2.0203\n",
      "step 1300: train loss 1.9369, val loss 1.9634\n",
      "step 1400: train loss 1.9100, val loss 1.9552\n",
      "step 1500: train loss 1.9054, val loss 1.9380\n",
      "step 1600: train loss 1.8480, val loss 1.9136\n",
      "step 1700: train loss 1.8453, val loss 1.9062\n",
      "step 1800: train loss 1.8397, val loss 1.8911\n",
      "step 1900: train loss 1.8253, val loss 1.8661\n",
      "step 2000: train loss 1.8108, val loss 1.8559\n",
      "step 2100: train loss 1.7824, val loss 1.8512\n",
      "step 2200: train loss 1.7848, val loss 1.8472\n",
      "step 2300: train loss 1.7670, val loss 1.8266\n",
      "step 2400: train loss 1.7441, val loss 1.8032\n",
      "step 2500: train loss 1.7428, val loss 1.8092\n",
      "step 2600: train loss 1.7363, val loss 1.8030\n",
      "step 2700: train loss 1.7230, val loss 1.7888\n",
      "step 2800: train loss 1.7121, val loss 1.7705\n",
      "step 2900: train loss 1.7063, val loss 1.7639\n",
      "step 3000: train loss 1.6899, val loss 1.7580\n",
      "step 3100: train loss 1.6991, val loss 1.7641\n",
      "step 3200: train loss 1.7061, val loss 1.7584\n",
      "step 3300: train loss 1.6855, val loss 1.7387\n",
      "step 3400: train loss 1.6715, val loss 1.7116\n",
      "step 3500: train loss 1.6630, val loss 1.7340\n",
      "step 3600: train loss 1.6607, val loss 1.7265\n",
      "step 3700: train loss 1.6532, val loss 1.7160\n",
      "step 3800: train loss 1.6454, val loss 1.7105\n",
      "step 3900: train loss 1.6443, val loss 1.7143\n",
      "step 4000: train loss 1.6323, val loss 1.6930\n",
      "step 4100: train loss 1.6348, val loss 1.7025\n",
      "step 4200: train loss 1.6318, val loss 1.7066\n",
      "step 4300: train loss 1.6240, val loss 1.6932\n",
      "step 4400: train loss 1.6280, val loss 1.6872\n",
      "step 4500: train loss 1.6214, val loss 1.6917\n",
      "step 4600: train loss 1.6048, val loss 1.6801\n",
      "step 4700: train loss 1.6120, val loss 1.6786\n",
      "step 4800: train loss 1.5977, val loss 1.6726\n",
      "step 4900: train loss 1.5996, val loss 1.6629\n",
      "step 4999: train loss 1.5895, val loss 1.6640\n",
      "Stopping time: 185.248900833\n",
      "Time taken:  2.86 minutes\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "print('Starting time:', start)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    # evaluate loss on training and test at various points\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluating loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Stopping time:', stop)\n",
    "print('Time taken: ', round((stop - start)/60.0, 2),'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a7bd6-3109-47d1-b437-4d1891ef9d30",
   "metadata": {},
   "source": [
    "### 7. Generative Pretrained Tolstoy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a25bf1-0457-4bec-8e7a-5eca96b38951",
   "metadata": {},
   "source": [
    "How good is our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6472394e-684d-4559-b5d4-adbed3514c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "“Austory have that explainry knew and\n",
      "the lafting to ‘y the kinds of your?”\n",
      "\n",
      "“Helvead in its porth art. Somet sert or this in the fact. “Sher of of all the soft, and been list liviked heorse.\n",
      "\n",
      "Emperor’s some of of the roun bridly that that them, as of mereive,\n",
      "“insel at that,” he began to but no evalle Moscow on the old goin it?”\n",
      "\n",
      "“I was what’s nothing who did not ar at as\n",
      "could it sometheoriers, was away soldiet Prince, ‘What days at whom think it shake fring that head ling what roe whástatin, why there\n",
      "sot—cornies.sER I!\n",
      "   C3\n",
      "\n",
      "\n",
      "Oh, to all went his not was long for the Emperora shoutinal get to her not Cave then the knich\n",
      "in wenther hossinution, everything majou.”\n",
      "\n",
      "“This’ with, he asking her hard been all yehearding the Sung art,\n",
      "to can, exther pervans.”\n",
      "\n",
      "Bertin that shouting-hoad (he sometthing his changry her you, the hume,\n",
      "but to ask wort, “and Irmóus for the dest.\n",
      "\n",
      "“What’s go his Might, only aftensionside to the gave Kutúzov.\n",
      "\n",
      "“Chas long extreminot when and the held Pzince Vasíkhov, gonst\n",
      "which?”\n",
      "    * * CHAPTER\n",
      "XXIXII\n",
      "\n",
      "   * Ausite,” desing Rostóvsues remariom a shoul, and her\n",
      "to her in she had ber face, am the Emporling, an the traiting\n",
      "of his should by Balught haven personed that the draw and\n",
      "of howinkna, and gently had failled tward the\n",
      "he have it hast!” said he head coulunt with the start the leaved\n",
      "of regreams, Pierre who, by sucome thought his no all here lees the half solein nents.\n",
      "As what’s apwer the crowd criving as off, with his Lushëv called became\n",
      "Borís on, had her has sabboon that the house\n",
      "was evight granned that women out that they sudding har behind a suchort, en a costake suddres, the movina Jerieus of\n",
      "a de Denísov, and too they in a\n",
      "Pávskhov Dakhove!.2 She dolds.” And tere this Sólul confervandes to be do shile\n",
      "them.\n",
      "\n",
      "Iim we’ll we a not still stillow the movinst of which a waitil most some a angain be not the conceoe his extruched\n",
      "noter might that in the His obger string day all the Prince Andrew,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask the GPT model to generate text!\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd2ee7d-3dda-47ab-b227-e37b602f07b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
